{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary material\n",
    "\n",
    "## This notebook represent results of Isolation forest applied on the coefficients after 107 different filtering. It take approximatively one hour of computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from time import *\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "os.chdir('../../Datasets')\n",
    "#######################################\n",
    "\n",
    "chinatown = pd.read_csv('China_Train.csv', header = None)\n",
    "chinatown2 = pd.read_csv('China_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X1_train = np.array(chinatown, dtype = float)[:,1:]\n",
    "X1_test = np.array(chinatown2, dtype = float)[:,1:]\n",
    "y1_train = np.array(chinatown, dtype=float)[:,0]\n",
    "y1_test = np.array(chinatown2, dtype=float)[:,0]\n",
    "\n",
    "coffee = pd.read_csv('Coffee_Train.csv', header = None)\n",
    "coffee2 = pd.read_csv('Coffee_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X2_train = np.array(coffee, dtype = float)[:,1:]\n",
    "X2_test = np.array(coffee2, dtype = float)[:,1:]\n",
    "y2_train = np.array(coffee, dtype=float)[:,0]\n",
    "y2_test = np.array(coffee2, dtype=float)[:,0]\n",
    "\n",
    "ecgfivedays = pd.read_csv('ECGFiveDays_Train.csv', header = None)\n",
    "ecgfivedays2 = pd.read_csv('ECGFiveDays_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X3_train = np.array(ecgfivedays, dtype = float)[:,1:]\n",
    "X3_test = np.array(ecgfivedays2, dtype = float)[:,1:]\n",
    "y3_train = np.array(ecgfivedays, dtype=float)[:,0]\n",
    "y3_test = np.array(ecgfivedays2, dtype=float)[:,0]\n",
    "\n",
    "ecg200 = pd.read_csv('ECG200_Train.csv', header = None)\n",
    "ecg200_2 = pd.read_csv('ECG200_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X4_train = np.array(ecg200, dtype = float)[:,1:]\n",
    "X4_test = np.array(ecg200_2, dtype = float)[:,1:]\n",
    "y4_train = np.array(ecg200, dtype=float)[:,0]\n",
    "y4_test = np.array(ecg200_2, dtype=float)[:,0]\n",
    "\n",
    "handoutlines = pd.read_csv('Handoutlines_Train.csv', header = None)\n",
    "handoutlines_2 = pd.read_csv('Handoutlines_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X5_train = np.array(handoutlines, dtype = float)[:,1:]\n",
    "X5_test = np.array(handoutlines_2, dtype = float)[:,1:]\n",
    "y5_train = np.array(handoutlines, dtype=float)[:,0]\n",
    "y5_test = np.array(handoutlines_2, dtype=float)[:,0]\n",
    "\n",
    "SonyRobotAI1 = pd.read_csv('SonyRobotAI1_Train.csv', header = None)\n",
    "SonyRobotAI1_2 = pd.read_csv('SonyRobotAI1_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X6_train = np.array(SonyRobotAI1, dtype = float)[:,1:]\n",
    "X6_test = np.array(SonyRobotAI1_2, dtype = float)[:,1:]\n",
    "y6_train = np.array(SonyRobotAI1, dtype=float)[:,0]\n",
    "y6_test = np.array(SonyRobotAI1_2, dtype=float)[:,0]\n",
    "\n",
    "\n",
    "SonyRobotAI2 = pd.read_csv('SonyRobotAI2_Train.csv', header = None)\n",
    "SonyRobotAI2_2 = pd.read_csv('SonyRobotAI2_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X7_train = np.array(SonyRobotAI2, dtype = float)[:,1:]\n",
    "X7_test = np.array(SonyRobotAI2_2, dtype = float)[:,1:]\n",
    "y7_train = np.array(SonyRobotAI2, dtype=float)[:,0]\n",
    "y7_test = np.array(SonyRobotAI2_2, dtype=float)[:,0]\n",
    "\n",
    "starlightcurves = pd.read_csv('StarLightCurves_Train.csv', header = None)\n",
    "starlightcurves2  = pd.read_csv('StarLightCurves_Test1.csv', header = None)\n",
    "starlightcurves3  = pd.read_csv('StarLightCurves_Test2.csv', header = None)\n",
    "\n",
    "\n",
    "X8_train = np.array(starlightcurves, dtype = float)[:,1:]\n",
    "y8_train = np.array(starlightcurves, dtype=float)[:,0]\n",
    "\n",
    "X8_test1 = np.array(starlightcurves2, dtype = float)[:,1:]\n",
    "X8_test2 = np.array(starlightcurves3, dtype = float)[:,1:]\n",
    "X8_test = np.concatenate((X8_test1,X8_test2), axis = 0)\n",
    "y8_test1 = np.array(starlightcurves2, dtype=float)[:,0]\n",
    "y8_test2 = np.array(starlightcurves3, dtype=float)[:,0]\n",
    "y8_test = np.concatenate((y8_test1,y8_test2))\n",
    "\n",
    "\n",
    "\n",
    "twoleadECG = pd.read_csv('TwoLeadECG_Train.csv', header = None)\n",
    "twoleadECG2  = pd.read_csv('TwoLeadECG_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X9_train = np.array(twoleadECG , dtype = float)[:,1:]\n",
    "X9_test = np.array(twoleadECG2 , dtype = float)[:,1:]\n",
    "y9_train = np.array(twoleadECG , dtype=float)[:,0]\n",
    "y9_test = np.array(twoleadECG2 , dtype=float)[:,0]\n",
    "\n",
    "yoga = pd.read_csv('Yoga_Train.csv', header = None)\n",
    "yoga2  = pd.read_csv('Yoga_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X10_train = np.array(yoga , dtype = float)[:,1:]\n",
    "X10_test = np.array(yoga2 , dtype = float)[:,1:]\n",
    "y10_train = np.array(yoga , dtype=float)[:,0]\n",
    "y10_test = np.array(yoga2 , dtype=float)[:,0]\n",
    "\n",
    "y10_train[np.where(y10_train == 1)[0]] = -1\n",
    "y10_train[np.where(y10_train == 2)[0]] = 1\n",
    "\n",
    "\n",
    "EOGHorizontal = pd.read_csv('EOGHorizontal_Train.csv', header = None)\n",
    "EOGHorizontal2  = pd.read_csv('EOGHorizontal_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X11_train = np.array(EOGHorizontal , dtype = float)[:,1:]\n",
    "X11_test = np.array(EOGHorizontal2 , dtype = float)[:,1:]\n",
    "y11_train = np.array(EOGHorizontal , dtype=float)[:,0]\n",
    "y11_test = np.array(EOGHorizontal2 , dtype=float)[:,0]\n",
    "\n",
    "\n",
    "\n",
    "CinECGTorso = pd.read_csv('CinECGTorso_Train.csv', header = None)\n",
    "CinECGTorso2  = pd.read_csv('CinECGTorso_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X12_train = np.array(CinECGTorso , dtype = float)[:,1:]\n",
    "X12_test = np.array(CinECGTorso2 , dtype = float)[:,1:]\n",
    "y12_train = np.array(CinECGTorso , dtype=float)[:,0]\n",
    "y12_test = np.array(CinECGTorso2 , dtype=float)[:,0]\n",
    "\n",
    "ECG5000 = pd.read_csv('ECG5000_Train.csv', header = None)\n",
    "ECG50002  = pd.read_csv('ECG5000_Test.csv', header = None)\n",
    "\n",
    "\n",
    "X13_train = np.array(ECG5000 , dtype = float)[:,1:]\n",
    "X13_test = np.array(ECG50002 , dtype = float)[:,1:]\n",
    "y13_train = np.array(ECG5000 , dtype=float)[:,0]\n",
    "y13_test = np.array(ECG50002 , dtype=float)[:,0]\n",
    "\n",
    "\n",
    "l = [[X1_train, X1_test, y1_test], [X2_train, X2_test, y2_test], [X3_train, X3_test, y3_test],\n",
    "\t   [X4_train, X4_test, y4_test], [X5_train, X5_test, y5_test], [X6_train, X6_test, y6_test],\n",
    "\t   [X7_train, X7_test, y7_test], [X8_train, X8_test, y8_test], [X9_train, X9_test, y9_test],\n",
    "\t   [X10_train, X10_test, y10_test], [X11_train, X11_test, y11_test], [X12_train, X12_test, y12_test],\n",
    "\t   [X13_train, X13_test, y13_test]]\n",
    "\n",
    "#################################\n",
    "#################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run filtering then Isolation Forest, Local Outlier Factor and One-class SVM :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier filtering :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier(dataset):\n",
    "    np.random.seed(42)\n",
    "    predict_IF = []\n",
    "    predict_LOF = []\n",
    "    predict_OCSVM = []\n",
    "    # we take the 30 fourier coefficients :\n",
    "    a = np.fft.fft(dataset[0], n = 30)\n",
    "    b = np.fft.fft(dataset[1], n = 30)\n",
    "    z_train = np.concatenate((a.real,a.imag), axis = 1)\n",
    "    z_test = np.concatenate((b.real,b.imag), axis = 1)\n",
    "    \n",
    "    # Isolation Forest :\n",
    "    IF = IsolationForest(n_estimators=100, max_samples='auto',behaviour='new', contamination='auto')\n",
    "    IF.fit(z_train)\n",
    "    s = IF.decision_function(z_test)\n",
    "    predict_IF.append(roc_auc_score(dataset[2], s))\n",
    "\n",
    "\n",
    "    # Local Outlier Factor :\n",
    "    LOF = LocalOutlierFactor(n_neighbors=20 ,contamination='auto', novelty = True)\n",
    "    LOF.fit(z_train)\n",
    "    s2 = LOF.decision_function(z_test)\n",
    "    predict_LOF.append(roc_auc_score(dataset[2], s2))\n",
    "\n",
    "\n",
    "    # One class SVM :\n",
    "    OC = OneClassSVM(kernel='rbf',gamma='scale')\n",
    "    OC.fit(z_train)\n",
    "    s3 = OC.decision_function(z_test)\n",
    "    predict_OCSVM.append(roc_auc_score(dataset[2], s3))\n",
    "\n",
    "    return predict_IF, predict_LOF, predict_OCSVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Parallelization :\n",
    "#################################\n",
    "if __name__ == '__main__': # excute on main process only\n",
    "\n",
    "    p = Pool(13)\n",
    "    result_filtering_fourier = p.map(fourier, l) \n",
    "score_IF_fourier = []\n",
    "score_LOF_fourier = []\n",
    "score_OCSVM_fourier = []\n",
    "for i in range(len(l)):\n",
    "    score_IF_fourier.append(result_filtering_fourier[i][0])\n",
    "    score_LOF_fourier.append(result_filtering_fourier[i][1])\n",
    "    score_OCSVM_fourier.append(result_filtering_fourier[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelets filterings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_base(dataset):\n",
    "\tnp.random.seed(42)\n",
    "\tliste = pywt.wavelist(kind='discrete')\n",
    "\tpredict_IF = []\n",
    "\tpredict_LOF = []\n",
    "\tpredict_OCSVM = []\n",
    "\tfor my_wave in liste:\n",
    "\n",
    "\t\t# Filtering of the training set :\n",
    "\t\tcoeffs_ = pywt.wavedec(dataset[0], my_wave, mode='sym')\n",
    "\t\tcoeffs = np.hstack(coeffs_)\n",
    "        # Filtering of the testing set :\n",
    "\t\tcoeffs2_ = pywt.wavedec(dataset[1], my_wave, mode='sym')\n",
    "\t\tcoeffs2 = np.hstack(coeffs2_)\n",
    "\n",
    "\t\t# Isolation Forest :\n",
    "\t\tIF = IsolationForest(n_estimators=100, max_samples='auto',behaviour='new', contamination='auto')\n",
    "\t\tIF.fit(coeffs)\n",
    "\t\ts = IF.decision_function(coeffs2)\n",
    "\t\tpredict_IF.append(roc_auc_score(dataset[2], s))\n",
    "\n",
    "\n",
    "\t\t# Local Outlier Factor :\n",
    "\t\tLOF = LocalOutlierFactor(n_neighbors=20 ,contamination='auto', novelty = True)\n",
    "\t\tLOF.fit(coeffs)\n",
    "\t\ts2 = LOF.decision_function(coeffs2)\n",
    "\t\tpredict_LOF.append(roc_auc_score(dataset[2], s2))\n",
    "\n",
    "\n",
    "\t\t# One class SVM :\n",
    "\t\tOC = OneClassSVM(kernel='rbf',gamma='scale')\n",
    "\t\tOC.fit(coeffs)\n",
    "\t\ts3 = OC.decision_function(coeffs2)\n",
    "\t\tpredict_OCSVM.append(roc_auc_score(dataset[2], s3))\n",
    "\n",
    "\treturn predict_IF, predict_LOF, predict_OCSVM\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "# Parallelization :\n",
    "#################################\n",
    "if __name__ == '__main__': # excute on main process only\n",
    "\n",
    "    p = Pool(13)\n",
    "    results_filtering = p.map(test_base, l) \n",
    "\n",
    "#################################\n",
    "\n",
    "#################################\n",
    "# Save results :\n",
    "#################################\n",
    "A = np.zeros((106,len(l)))\n",
    "B = np.zeros((106,len(l)))\n",
    "C = np.zeros((106,len(l)))\n",
    "\n",
    "# Isolation Forest :\n",
    "for i in range(len(l)):\n",
    "\tA[:,i] = results_filtering[i][0]\n",
    "\n",
    "# Local Outlier Factor :\n",
    "for i in range(len(l)):\n",
    "\tB[:,i] = results_filtering[i][1]\n",
    "\n",
    "# One class SVM :\n",
    "for i in range(len(l)):\n",
    "\tC[:,i] = results_filtering[i][2]  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
